{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Now that we have our cleaned data with required features, lets proceed with model development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "#import lightgbm as ltb\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties = pd.read_csv('cleaned_property_data_buy.csv')\n",
    "#df_properties = df_properties.dropna(column='amenities',axis=1)\n",
    "df_properties = df_properties.drop(columns = ['amenities','price_per_sqft','neighborhood'],axis=1)\n",
    "\n",
    "df_properties = pd.get_dummies(df_properties, columns=['quality', ])\n",
    "df_properties.head()\n",
    "#partly_furnished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_properties.price = df_properties.price * 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_properties.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target feature for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_properties['price']#.values\n",
    "#type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding target feature from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_properties.drop(columns=['price','id'],axis=1)\n",
    "#X['maid_room'] = df[\"maid_room\"].astype(int)\n",
    "#X['maid_room'] = X['maid_room'].replace({True: 1, False: 0})\n",
    "X = X.replace({False: 0, True: 1}, inplace=False)\n",
    "#unfurnished\n",
    "#X['partly_furnished'] = X['partly_furnished'].replace({1: 0, 0: 1})\n",
    "#X = X[['latitude','longitude','size_in_sqft','no_of_bedrooms','no_of_bathrooms','covered_parking','unfurnished','concierge','kitchen_appliances','pets_allowed','view_of_water']]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction modelling we will use following models:\n",
    "    \n",
    "- LGBMRegressor\n",
    "- RandomForestRegressor\n",
    "- XGBRegressor\n",
    "- LGBMRegressor\n",
    "\n",
    "We will train our models using training dataset consisting of 1619 properties and then test them against our testing dataset with 2024-1619 properties.\n",
    "\n",
    "We will check our R2 score for each model and we will select the predictions from the model which is closest to value of 1. R2 scores range from 0 to 1.\n",
    "\n",
    "Based on best R2 score we will match actual values with predicted values and see the percentage difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#np.random.seed(42)\n",
    "\n",
    "for i in [0.001, 0.003, 0.1, 0.3, 1, 2, 4, 6]:\n",
    "    for j in [100, 250, 500, 700, 750, 800, 850, 900, 950, 1000, 1050]:\n",
    "       gradient = GradientBoostingRegressor(n_estimators = j, learning_rate=i)\n",
    "       print('learning rate is: '+ str(i)+ ' and n_estimators is: '+str(j))\n",
    "       gradient.fit(X_train, y_train)\n",
    "       gradient_predictions = gradient.predict(X_test)\n",
    "       gradient_r2_score = r2_score(y_test, gradient_predictions)\n",
    "       print('R2 Score for GradientBoostingRegressor', gradient_r2_score)\n",
    "       \n",
    "       print(\"MAE is\",mean_absolute_error(y_test, gradient_predictions))\n",
    "       print(\"MAPE is\",mean_absolute_percentage_error(y_test, gradient_predictions))\n",
    "       print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient_predictions = gradient.predict(df_test)\n",
    "gradient = GradientBoostingRegressor(n_estimators = 750, learning_rate=0.3)\n",
    "gradient.fit(X_train, y_train)\n",
    "       \n",
    "gradient_predictions = gradient.predict(X_test)\n",
    "gradient_r2_score = r2_score(y_test, gradient_predictions)\n",
    "print('R2 Score for GradientBoostingRegressor', gradient_r2_score)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE\",mean_absolute_error(y_test, gradient_predictions))\n",
    "print(\"MAPE is\",mean_absolute_percentage_error(y_test, gradient_predictions))\n",
    "filename = 'GradientBoostingRegressor2_buy.sav'\n",
    "pickle.dump(gradient, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
    "gradient = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gradient.fit(train[predictors],train[target])\n",
    "gradient.grid_scores_, gradient.best_params_, gradient.best_score_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]:\n",
    "       random_forest = RandomForestRegressor(max_depth = j)#, learning_rate=i)\n",
    "       print(' and max_depth is: '+str(j))\n",
    "       random_forest.fit(X_train, y_train)\n",
    "       forest_prediction = random_forest.predict(X_test)\n",
    "       forest_r2_score = r2_score(y_test, forest_prediction)\n",
    "       print('R2 Score for RandomForestRegressor', forest_r2_score)\n",
    "       \n",
    "       print(\"MAE\",mean_absolute_error(y_test, forest_prediction))\n",
    "       print(\"MAPE is\",mean_absolute_percentage_error(y_test, forest_prediction))\n",
    "       print('\\n')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(max_depth=16)\n",
    "random_forest.fit(X_train, y_train)\n",
    "#forest_prediction = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_prediction = random_forest.predict(X_test)\n",
    "forest_r2_score = r2_score(y_test, forest_prediction)\n",
    "print('R2 Score for RandomForestRegressor', forest_r2_score)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"MAE\",mean_absolute_error(y_test, forest_prediction))\n",
    "print(\"MAPE is\",mean_absolute_percentage_error(y_test, forest_prediction))\n",
    "\n",
    "filename = 'random_forest2_buy.sav'\n",
    "pickle.dump(random_forest, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.001, 0.003, 0.1, 0.3, 1, 2, 4, 6]:\n",
    "    for j in [50, 100, 250, 500, 700, 750, 800, 850, 900, 950, 1000, 1050]:\n",
    "       xgbr_regressor = XGBRegressor(learning_rate=i, n_estimators=j, n_jobs=-1)\n",
    "       xgbr_regressor.fit(X_train, y_train)\n",
    "       \n",
    "       xgbr_regressor = random_forest.predict(X_test)\n",
    "       xgbreg_r2_score = r2_score(y_test, xgbr_regressor)\n",
    "       print('learning rate is '+str(i)+' nestimators is '+str(j))\n",
    "       print('R2 Score for RandomForestRegressor', xgbreg_r2_score)\n",
    "       \n",
    "       print(\"MAE\",mean_absolute_error(y_test, xgbr_regressor))\n",
    "       print(\"MAPE is\",mean_absolute_percentage_error(y_test, xgbr_regressor))\n",
    "       print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_regressor = XGBRegressor(learning_rate=0.1, n_estimators=100, n_jobs=-1)\n",
    "xgbr_regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xgbr_regressor.feature_importances_\n",
    "xgbr_regressor.feature_names_in_\n",
    "\n",
    "fig = plt.figure(figsize=(20, 25))\n",
    "plt.barh(xgbr_regressor.feature_names_in_, xgbr_regressor.feature_importances_)\n",
    "plt.xlabel(\"Courses offered\")\n",
    "plt.ylabel(\"No. of students enrolled\")\n",
    "plt.title(\"Students enrolled in different courses\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_regressor_prediction = xgbr_regressor.predict(X_test)\n",
    "xgbr_regresso_r2_score = r2_score(y_test, xgbr_regressor_prediction)\n",
    "print('R2 Score for XGBRegressor', xgbr_regresso_r2_score)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"MAE\",mean_absolute_error(y_test, xgbr_regressor_prediction))\n",
    "\n",
    "print(\"MAPE is\",mean_absolute_percentage_error(y_test, xgbr_regressor_prediction))\n",
    "\n",
    "filename = 'xgbr_regressor2_buy.sav'\n",
    "pickle.dump(xgbr_regressor, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(115, 95))\n",
    "plot_importance(xgbr_regressor,height=25.2)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "feature_important = xgbr_regressor.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(40, columns=\"score\").plot(kind='barh', figsize = (30,20)) ## plot top 40 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = pd.DataFrame(columns=[X.columns.values,'bonjour'])\n",
    "data.loc['latitude']=1.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import gradio as gr\n",
    "\n",
    "def greet(latitude=0,longitude=0,size_in_sqft=0,no_of_bedrooms=1,no_of_bathrooms=1, view_of_water=True, unfurnished=False,covered_parking=False,pets_allowed=False, kitchen_appliances=False, concierge=False):\n",
    "    print('bonjour')\n",
    "    tab =['latitude','longitude','size_in_sqft','no_of_bedrooms','no_of_bathrooms','covered_parking','unfurnished','concierge','kitchen_appliances','pets_allowed','view_of_water']\n",
    "    print(latitude)\n",
    "    print(longitude)\n",
    "    print(unfurnished)\n",
    "    dt = pd.DataFrame({'latitude': [latitude],\n",
    "                        'longitude': [longitude],\n",
    "                        'size_in_sqft':  [size_in_sqft],\n",
    "                        'no_of_bedrooms':  [no_of_bedrooms],\n",
    "                        'no_of_bathrooms':  [no_of_bathrooms],\n",
    "                        'view_of_water':  [1] if view_of_water==True else [0],\n",
    "                        'unfurnished':  [1] if unfurnished==True else [0],\n",
    "                        'covered_parking':  [1] if covered_parking==True else [0],\n",
    "                        'pets_allowed': [1] if pets_allowed==True else [0],\n",
    "                        'kitchen_appliances': [1] if kitchen_appliances==True else [0],\n",
    "                        'concierge': [1] if concierge==True else [0]\n",
    "                        } )\n",
    "    \n",
    "         #print('X.columns',X.columns)\n",
    "    \n",
    "    dt.head        \n",
    "    print(dt['latitude'])\n",
    "    print(X.shape)\n",
    "    \n",
    "    \n",
    "    filename = 'GradientBoostingRegressor2.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.predict(dt)\n",
    "    result = result[0] *0.27\n",
    "         \n",
    "    print('result',result) \n",
    "    #dt.head()\n",
    "    return result   \n",
    "    #gradient_predictions = gradient.predict(tab)\n",
    "    #return gradient_predictions\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"number\", \"number\", \"number\", \"number\",\"number\",\"checkbox\",\"checkbox\",\"checkbox\",\"checkbox\",\"checkbox\",\"checkbox\"],\n",
    "    outputs=[\"number\"],\n",
    ")\n",
    "demo.launch()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras\n",
    "from tensorflow import keras\n",
    "from PIL import ImageFont\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "#NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "#NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"history = NN_model.fit(X, y, epochs=200, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame.from_dict(history.history)\n",
    "sns.lineplot(data=history_df[['mean_absolute_error', 'val_mean_absolute_error']])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MAE\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
